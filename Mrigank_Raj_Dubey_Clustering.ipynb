{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8Q-f6RVrs_y"
      },
      "outputs": [],
      "source": [
        "# Mrigank Raj Dubey - Customer Segmentation (Clustering) for Data Science Assignment\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load datasets\n",
        "customers = pd.read_csv('Customers.csv')\n",
        "transactions = pd.read_csv('Transactions.csv')\n",
        "\n",
        "# Merge datasets\n",
        "data = transactions.merge(customers, on='CustomerID')\n",
        "\n",
        "# Feature Engineering\n",
        "customer_features = data.groupby('CustomerID').agg({\n",
        "    'TotalValue': 'sum',   # Total spending\n",
        "    'TransactionID': 'count',  # Total number of transactions\n",
        "    'ProductID': 'nunique'  # Product diversity\n",
        "}).reset_index()\n",
        "\n",
        "# Rename columns for clarity\n",
        "customer_features.rename(columns={\n",
        "    'TotalValue': 'TotalSpending',\n",
        "    'TransactionID': 'TotalTransactions',\n",
        "    'ProductID': 'UniqueProducts'\n",
        "}, inplace=True)\n",
        "\n",
        "# Normalize data\n",
        "scaler = StandardScaler()\n",
        "normalized_features = scaler.fit_transform(customer_features.iloc[:, 1:])\n",
        "\n",
        "# Perform K-Means Clustering\n",
        "# Experiment with different cluster sizes (2 to 10)\n",
        "k_values = range(2, 11)\n",
        "db_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(normalized_features)\n",
        "    db_index = davies_bouldin_score(normalized_features, labels)\n",
        "    db_scores.append(db_index)\n",
        "\n",
        "# Choose the optimal number of clusters (lowest DB Index)\n",
        "optimal_k = k_values[db_scores.index(min(db_scores))]\n",
        "final_kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "final_labels = final_kmeans.fit_predict(normalized_features)\n",
        "\n",
        "# Add cluster labels to customer features\n",
        "data_with_clusters = customer_features.copy()\n",
        "data_with_clusters['Cluster'] = final_labels\n",
        "\n",
        "# Visualize Clusters (Using PCA for dimensionality reduction)\n",
        "pca = PCA(n_components=2)\n",
        "pca_features = pca.fit_transform(normalized_features)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=pca_features[:, 0], y=pca_features[:, 1], hue=final_labels, palette='Set1')\n",
        "plt.title('Customer Segments (PCA Visualization)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# Save results to CSV\n",
        "data_with_clusters.to_csv('Mrigank_Raj_Dubey_Clustering.csv', index=False)\n",
        "\n",
        "# Print clustering results\n",
        "print(f\"Optimal Number of Clusters: {optimal_k}\")\n",
        "print(f\"Davies-Bouldin Index: {min(db_scores):.2f}\")\n",
        "print(\"Clustered data saved to 'Mrigank_Raj_Dubey_Clustering.csv'\")\n"
      ]
    }
  ]
}